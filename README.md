# Face Blur

## Overview

This is a small project using a neural network to locate and then blur your face.

## Requirements



### Dependencies

To get started, you'll need to install the required libraries. You can install them using `pip`:

```bash
pip install -r requirements.txt
```
## Reccomended
### GPU
Any NVIDIA GPU will do better than most AMD or Intel GPU. NVIDIA has Compute Unified Device Architecture or CUDA which allows it to have better performance with neural networks. This will increase the speed of which your neural network will run and the better it will blur your face.
<div>
  <img src="./assets/nvidia_training.png" width="700" height="350">
  
</div>
This shows the training performance of lots of consumer cards of NVIDIA, AMD and Intel. Bear in mind the higher score, the better it will perform in training which includes epochs, processing and most other things.

### When to increase / decrease the dataset (amount of images)
Increase Dataset Size When:

    
    There‚Äôs a large gap between training and validation accuracy ‚Äî overfitting warning sign.
    You‚Äôre seeing unstable validation accuracy/loss ‚Äî could be due to lack of variety in your data.
    Your model performs well on training data but poorly in the real world.
    You‚Äôre using a complex model (like deep CNNs) ‚Äî they need lots of data to generalize well.

1. Your model achieves high accuracy very quickly like the image below
<img src="./assets/high_accuracy.png" width="200" height="175">
This basically means that your model is too smart for the amount of data your giving it (giving a spelling book to someone in college etc).


2Ô∏è‚É£ Number of Epochs
üîº Increase Epochs When:

    The model is still learning ‚Äî loss is decreasing steadily, and accuracy is still improving.
    You‚Äôre using data augmentation (it slows down learning a bit, so more epochs help).
    You‚Äôre training on a larger dataset or using a deeper model.
    You use regularization (like dropout, weight decay) ‚Äî helps avoid overfitting but often needs more epochs to converge.

üîç Example:

If loss is still dropping at epoch 15 and accuracy is improving, training longer may help.
üîΩ Decrease Epochs When:

    Your model overfits quickly (loss on validation increases while training loss decreases).
    Training and validation accuracy plateau early, like in your case ‚Äî no need to go on.
    You use early stopping ‚Äî this automatically stops training when validation performance stops improving.
    You‚Äôre running a quick prototype or test.

‚öñÔ∏è Balance: Dataset vs Epochs
Small Dataset	Big Dataset
Needs fewer epochs (model memorizes quickly)	Needs more epochs (model takes longer to generalize)
High risk of overfitting	Better generalization
Data augmentation recommended	May or may not need augmentation
Few Epochs	Many Epochs
Good for fast testing	Better for full training
Risk underfitting on large datasets	Risk overfitting on small datasets
Combine with early stopping	Combine with regularization
üîë Rules of Thumb:
Situation	What to Do
Fast overfitting, high accuracy early	Add more data or reduce epochs
Training and validation both low	Increase data, epochs, or model complexity
Validation accuracy stagnant or dropping	Use early stopping, reduce epochs, or regularize
Loss still decreasing, accuracy rising	Increase epochs
Using data augmentation	Increase epochs to allow more learning
üí° Tips:

    Always visualize your learning curves like you did ‚Äî they tell you exactly what‚Äôs happening.
    Combine data size, model complexity, epochs, and regularization for best results.
    Try early stopping with patience=3-5 epochs as a safety net.

If you‚Äôd like, I can analyze your current training logs and suggest a new training strategy (epochs, batch size, augmentation setup). Want that?
