# Face Blur

## Overview

This is a small project using a neural network to locate and then blur your face.

## Requirements



### Dependencies

To get started, you'll need to install the required libraries. You can install them using `pip`:

```bash
pip install -r requirements.txt
```
## Reccomended
### GPU
Any NVIDIA GPU will do better than most AMD or Intel GPU. NVIDIA has Compute Unified Device Architecture or CUDA which allows it to have better performance with neural networks. This will increase the speed of which your neural network will run and the better it will blur your face.
<div>
  <img src="./assets/nvidia_training.png" width="700" height="350">
  
</div>
This shows the training performance of lots of consumer cards of NVIDIA, AMD and Intel. Bear in mind the higher score, the better it will perform in training which includes epochs, processing and most other things.

### When to increase / decrease the dataset (amount of images)
Increase Dataset Size When:

    
    Thereâ€™s a large gap between training and validation accuracy â€” overfitting warning sign.
    Youâ€™re seeing unstable validation accuracy/loss â€” could be due to lack of variety in your data.
    Your model performs well on training data but poorly in the real world.
    Youâ€™re using a complex model (like deep CNNs) â€” they need lots of data to generalize well.

1. Your model achieves high accuracy very quickly â€” this means itâ€™s likely memorizing.
<img src="">

2ï¸âƒ£ Number of Epochs
ğŸ”¼ Increase Epochs When:

    The model is still learning â€” loss is decreasing steadily, and accuracy is still improving.
    Youâ€™re using data augmentation (it slows down learning a bit, so more epochs help).
    Youâ€™re training on a larger dataset or using a deeper model.
    You use regularization (like dropout, weight decay) â€” helps avoid overfitting but often needs more epochs to converge.

ğŸ” Example:

If loss is still dropping at epoch 15 and accuracy is improving, training longer may help.
ğŸ”½ Decrease Epochs When:

    Your model overfits quickly (loss on validation increases while training loss decreases).
    Training and validation accuracy plateau early, like in your case â€” no need to go on.
    You use early stopping â€” this automatically stops training when validation performance stops improving.
    Youâ€™re running a quick prototype or test.

âš–ï¸ Balance: Dataset vs Epochs
Small Dataset	Big Dataset
Needs fewer epochs (model memorizes quickly)	Needs more epochs (model takes longer to generalize)
High risk of overfitting	Better generalization
Data augmentation recommended	May or may not need augmentation
Few Epochs	Many Epochs
Good for fast testing	Better for full training
Risk underfitting on large datasets	Risk overfitting on small datasets
Combine with early stopping	Combine with regularization
ğŸ”‘ Rules of Thumb:
Situation	What to Do
Fast overfitting, high accuracy early	Add more data or reduce epochs
Training and validation both low	Increase data, epochs, or model complexity
Validation accuracy stagnant or dropping	Use early stopping, reduce epochs, or regularize
Loss still decreasing, accuracy rising	Increase epochs
Using data augmentation	Increase epochs to allow more learning
ğŸ’¡ Tips:

    Always visualize your learning curves like you did â€” they tell you exactly whatâ€™s happening.
    Combine data size, model complexity, epochs, and regularization for best results.
    Try early stopping with patience=3-5 epochs as a safety net.

If youâ€™d like, I can analyze your current training logs and suggest a new training strategy (epochs, batch size, augmentation setup). Want that?
